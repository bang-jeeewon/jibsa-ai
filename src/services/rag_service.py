import pandas as pd
from pathlib import Path
from src.config.config import OPENAI_API_KEY
from openai import OpenAI

# ë¶„ë¦¬ëœ ëª¨ë“ˆ import
from src.services.rag.pdf_extractor import PDFExtractor
from src.services.rag.data_processor import DataProcessor
from src.services.rag.text_chunker import TextChunker
from src.services.rag.vector_store import VectorStoreService

openai = OpenAI(api_key=OPENAI_API_KEY)

class RAGService:
    def __init__(self, persist_directory='./data/vector_store'):
        """
        RAG íŒŒì´í”„ë¼ì¸ì„ ì´ê´„í•˜ëŠ” ì„œë¹„ìŠ¤ í´ë˜ìŠ¤.
        ETL í”„ë¡œì„¸ìŠ¤ë¥¼ ê° ë‹´ë‹¹ í´ë˜ìŠ¤ì—ê²Œ ìœ„ì„í•˜ì—¬ ì‹¤í–‰í•©ë‹ˆë‹¤.
        """
        # ê° ë‹¨ê³„ë³„ ë‹´ë‹¹ì(Worker) ì´ˆê¸°í™”
        self.pdf_extractor = PDFExtractor()
        self.data_processor = DataProcessor()
        self.text_chunker = TextChunker()
        self.vector_store = VectorStoreService(persist_directory)


    def process_pdf_for_rag(self, pdf_path: str, doc_id: str):
        """
        PDF íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ RAG ì‹œìŠ¤í…œì— ì ì¬í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜ ë° ì €ì¥í•©ë‹ˆë‹¤.
        :param pdf_path: PDF íŒŒì¼ ê²½ë¡œ
        :param doc_id: ë¬¸ì„œë¥¼ ì‹ë³„í•  ìˆ˜ ìˆëŠ” ê³ ìœ  ID (ì˜ˆ: house_manage_no)
        """
        # 1. Extract: PDFì—ì„œ Raw ë°ì´í„° ì¶”ì¶œ
        print(f"ğŸ” PDF ì¶”ì¶œ ì‹œì‘: {pdf_path}")
        raw_content = self.pdf_extractor.extract_content(pdf_path)
        
        # (ë””ë²„ê¹…ìš©) ì¶”ì¶œëœ í‘œ ë°ì´í„° ì—‘ì…€ ì €ì¥
        self.save_tables_to_excel(raw_content)
        
        # 2. Transform: ë°ì´í„° ì •ì œ ë° ë§ˆí¬ë‹¤ìš´ ë³€í™˜
        print("ğŸ§¹ ë°ì´í„° ì •ì œ ë° ë³€í™˜ ì¤‘...")
        processed_docs = self.data_processor.process_content(raw_content)
        
        # 3. Load (Temporary): íŒŒì¼ë¡œ ì €ì¥ (ì¶”í›„ Vector DB ì €ì¥ìœ¼ë¡œ ë³€ê²½)
        final_rag_document = "\n\n".join(processed_docs)
        self.save_rag_document_as_md(pdf_path, final_rag_document)
        
        # 4. Chunking: í…ìŠ¤íŠ¸ ì²­í‚¹
        print("ğŸ”ª í…ìŠ¤íŠ¸ ì²­í‚¹ ì¤‘...")
        chunks = self.text_chunker.chunk_markdown(final_rag_document)
        
        # [ì¤‘ìš”] ëª¨ë“  ì²­í¬ì— ë¬¸ì„œ ID(doc_id) ë©”íƒ€ë°ì´í„° ì¶”ê°€
        for chunk in chunks:
            chunk.metadata['doc_id'] = str(doc_id)

        print(f"âœ… ì´ {len(chunks)}ê°œì˜ ì²­í¬ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # (ë””ë²„ê¹…ìš©) ì²« ë²ˆì§¸ ì²­í¬ ë‚´ìš© ì¶œë ¥
        if chunks:
            print(f"ğŸ” ì²« ë²ˆì§¸ ì²­í¬ ì˜ˆì‹œ:\n{chunks[0].page_content[:200]}...")
            print(f"ğŸ”– ë©”íƒ€ë°ì´í„°: {chunks[0].metadata}")
            
            # 5. Load: ë²¡í„° DB ì €ì¥
            self.vector_store.add_documents(chunks)

        return '====ì²˜ë¦¬ ì™„ë£Œ===='

    def answer_question(self, question: str, doc_id: str = None):
        """
        ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ RAG ë°©ì‹ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
        :param doc_id: íŠ¹ì • ë¬¸ì„œì—ì„œë§Œ ê²€ìƒ‰í•˜ë ¤ë©´ ID ì§€ì •
        """
        print(f"ğŸ¤” ì§ˆë¬¸ ë¶„ì„ ì¤‘: {question} (doc_id: {doc_id})")
        
        # 1. Retrieve: ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ (í•„í„° ì ìš©)
        filter_condition = {"doc_id": str(doc_id)} if doc_id else None
        related_docs = self.vector_store.search(query=question, k=5, filter=filter_condition) 
        
        if not related_docs:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ê³µê³ ë¬¸ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        # 2. Augment: í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        context = "\n\n".join([doc.page_content for doc in related_docs])
        
        system_prompt = f"""
        ë‹¹ì‹ ì€ ì•„íŒŒíŠ¸ ì²­ì•½ ê³µê³ ë¬¸ì„ ì „ë¬¸ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
        ì•„ë˜ ì œê³µëœ [ê³µê³ ë¬¸ ë‚´ìš©]ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.
        
        - [ê³µê³ ë¬¸ ë‚´ìš©]ì— ì—†ëŠ” ì •ë³´ë¼ë©´, ì¶”ì¸¡í•˜ì§€ ë§ê³  "ê³µê³ ë¬¸ ë‚´ìš©ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.
        - ë‹µë³€ì€ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•˜ì—¬ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•˜ì„¸ìš”.
        - í‘œ í˜•ì‹ì˜ ë°ì´í„°ê°€ ìˆë‹¤ë©´ í•„ìš” ì‹œ í‘œë‚˜ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì •ë¦¬í•´ì„œ ë³´ì—¬ì£¼ì„¸ìš”.

        [ê³µê³ ë¬¸ ë‚´ìš©]
        {context}
        """

        # 3. Generate: ë‹µë³€ ìƒì„±
        try:
            response = openai.chat.completions.create(
                model="gpt-4o-mini", # ê°€ì„±ë¹„ ì¢‹ì€ ëª¨ë¸ ì‚¬ìš© (í•„ìš” ì‹œ gpt-4o ë³€ê²½ ê°€ëŠ¥)
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": question}
                ],
                temperature=0, # ì‚¬ì‹¤ ê¸°ë°˜ ë‹µë³€ì„ ìœ„í•´ 0 ì„¤ì •
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

    def clear_database(self):
        """ë²¡í„° DBë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        print("ğŸ—‘ï¸ ë²¡í„° DB ì´ˆê¸°í™” ìš”ì²­")
        self.vector_store.clear()

    def save_tables_to_excel(self, all_content, output_path="extracted_tables.xlsx"):
        """
        (ë””ë²„ê¹…ìš©) ì¶”ì¶œëœ í‘œ ë°ì´í„°ë¥¼ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥
        """
        with pd.ExcelWriter(output_path, engine="openpyxl") as writer:
            table_count = 0
            for item in all_content:
                if item["type"] == "table":
                    table_count += 1
                    table_data = item["content"]
                    
                    # ë°ì´í„°ê°€ ì—†ê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•Šìœ¼ë©´ íŒ¨ìŠ¤
                    if not table_data: continue
                    
                    df = pd.DataFrame(table_data)
                    sheet_name = f"Table {table_count}"
                    # ì‹œíŠ¸ ì´ë¦„ ê¸¸ì´ ì œí•œ (31ì)
                    if len(sheet_name) > 31: sheet_name = sheet_name[:31]
                    
                    try:
                        df.to_excel(writer, sheet_name=sheet_name, index=False)
                    except Exception as e:
                        print(f"ì—‘ì…€ ì‹œíŠ¸ ì €ì¥ ì‹¤íŒ¨ ({sheet_name}): {e}")

        print(f"âœ… í‘œ ë°ì´í„° ì—‘ì…€ ì €ì¥ ì™„ë£Œ: {output_path}")

    def save_rag_document_as_md(self, pdf_path: str, final_rag_document: str):
        """
        ìµœì¢… ë³€í™˜ëœ ë¬¸ì„œë¥¼ .md íŒŒì¼ë¡œ ì €ì¥
        """
        original_stem = Path(pdf_path).stem 
        md_filename = original_stem + ".md"
        output_dir = "data/md" 
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        output_path = Path(output_dir) / md_filename

        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(final_rag_document)
            print(f"âœ… ìµœì¢… Markdown ì €ì¥ ì™„ë£Œ: {output_path}")
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
