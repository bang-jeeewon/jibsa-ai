import pandas as pd
from pathlib import Path
from src.config.config import OPENAI_API_KEY, GOOGLE_API_KEY
from openai import OpenAI
from google.genai import Client
import time
import random
import re

# ë¶„ë¦¬ëœ ëª¨ë“ˆ import
from src.services.rag.pdf_extractor import PDFExtractor
# from src.services.rag.pdf_extractor_pymupdf import PDFExtractorPyMuPDF
# from src.services.rag.pdf_extractor_llama import PDFExtractorLlama
# from src.services.rag.pdf_extractor_marker import PDFExtractorMarker
from src.services.rag.data_processor import DataProcessor
from src.services.rag.text_chunker import TextChunker
from src.services.rag.vector_store import VectorStoreService

openai = OpenAI(api_key=OPENAI_API_KEY)
genai_client = Client(api_key=GOOGLE_API_KEY) 

class RAGService:
    def __init__(self, persist_directory=None, embedding_model="openai"):
        """
        RAG íŒŒì´í”„ë¼ì¸ì„ ì´ê´„í•˜ëŠ” ì„œë¹„ìŠ¤ í´ë˜ìŠ¤.
        ETL í”„ë¡œì„¸ìŠ¤ë¥¼ ê° ë‹´ë‹¹ í´ë˜ìŠ¤ì—ê²Œ ìœ„ì„í•˜ì—¬ ì‹¤í–‰í•©ë‹ˆë‹¤.
        :param persist_directory: Noneì´ë©´ in-memory ëª¨ë“œ (íŒŒì¼ ì €ì¥ ì•ˆ í•¨, ì„œë²„ ì¬ì‹œì‘ ì‹œ ë°ì´í„° ì‚¬ë¼ì§)
        :param embedding_model: ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ ("openai" ë˜ëŠ” "gemini")
        """
        # ê° ë‹¨ê³„ë³„ ë‹´ë‹¹ì(Worker) ì´ˆê¸°í™”
        self.pdf_extractor = PDFExtractor()
        # self.pdf_extractor_pymupdf = PDFExtractorPyMuPDF()
        # self.pdf_extractor_llama = PDFExtractorLlama()
        # self.pdf_extractor_marker = PDFExtractorMarker()
        self.data_processor = DataProcessor()
        self.text_chunker = TextChunker()
        self.vector_store = VectorStoreService(persist_directory, embedding_model=embedding_model)  # None = in-memory


    def process_pdf_for_rag(self, pdf_path: str, doc_id: str):
        """
        PDF íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ RAG ì‹œìŠ¤í…œì— ì ì¬í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜ ë° ì €ì¥í•©ë‹ˆë‹¤.
        :param pdf_path: PDF íŒŒì¼ ê²½ë¡œ
        :param doc_id: ë¬¸ì„œë¥¼ ì‹ë³„í•  ìˆ˜ ìˆëŠ” ê³ ìœ  ID (ì˜ˆ: house_manage_no)
        """
        # 1. Extract: PDFì—ì„œ Raw ë°ì´í„° ì¶”ì¶œ
        print(f"ğŸ” PDF ì¶”ì¶œ ì‹œì‘: {pdf_path}")
        raw_content = self.pdf_extractor.extract_content(pdf_path)
        # raw_content = self.pdf_extractor_pymupdf.extract_content(pdf_path)
        # raw_content = self.pdf_extractor_llama.extract_content(pdf_path)
        # raw_content = self.pdf_extractor_marker.extract_content(pdf_path)
        
        # (ë””ë²„ê¹…ìš©) ì¶”ì¶œëœ í‘œ ë°ì´í„° ì—‘ì…€ ì €ì¥
        # self.save_tables_to_excel(raw_content)
        
        # 2. Transform: ë°ì´í„° ì •ì œ ë° ë§ˆí¬ë‹¤ìš´ ë³€í™˜
        print("ğŸ§¹ ë°ì´í„° ì •ì œ ë° ë³€í™˜ ì¤‘...")
        processed_docs = self.data_processor.process_content(raw_content)
        
        # 3. ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œ ìƒì„± (íŒŒì¼ ì €ì¥ ì—†ì´ ë©”ëª¨ë¦¬ì—ì„œë§Œ ì²˜ë¦¬)
        final_rag_document = "\n\n".join(processed_docs)
        # íŒŒì¼ ì €ì¥ ì œê±°: self.save_rag_document_as_md(pdf_path, final_rag_document)
        
        # 4. Chunking: í…ìŠ¤íŠ¸ ì²­í‚¹
        print("ğŸ”ª í…ìŠ¤íŠ¸ ì²­í‚¹ ì¤‘...")
        chunks = self.text_chunker.chunk_markdown(final_rag_document)
        
        # [ì¤‘ìš”] ëª¨ë“  ì²­í¬ì— ë¬¸ì„œ ID(doc_id) ë©”íƒ€ë°ì´í„° ì¶”ê°€
        for chunk in chunks:
            chunk.metadata['doc_id'] = str(doc_id)

        print(f"âœ… ì´ {len(chunks)}ê°œì˜ ì²­í¬ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # (ë””ë²„ê¹…ìš©) ì²« ë²ˆì§¸ ì²­í¬ ë‚´ìš© ì¶œë ¥
        if chunks:
            # 5. Load: ë²¡í„° DB ì €ì¥
            self.vector_store.add_documents(chunks)

        return '====ì²˜ë¦¬ ì™„ë£Œ===='

    def answer_question(self, question: str, doc_id: str = None, model: str = "openai"):
        """
        ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ RAG ë°©ì‹ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
        :param doc_id: íŠ¹ì • ë¬¸ì„œì—ì„œë§Œ ê²€ìƒ‰í•˜ë ¤ë©´ ID ì§€ì •
        :param model: ì‚¬ìš©í•  ëª¨ë¸ ("openai" ë˜ëŠ” "gemini")
        """
        model_display = "GPT-4o-mini" if model == "openai" else "Gemini Pro"
        print(f"ğŸ¤” ì§ˆë¬¸ ë¶„ì„ ì¤‘: {question}")
        print(f"ğŸ“‹ ë¬¸ì„œ ID: {doc_id}, ì„ íƒëœ LLM: {model_display}")
        # 1íšŒ ì§ˆë¬¸ ë¹„ìš© ê³„ì‚° (k=5 ê¸°ì¤€):
        # - ì§ˆë¬¸: ì•½ 50 í† í°
        # - ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ (k=5): ì²­í¬ 5ê°œ Ã— 250 í† í° = ì•½ 1,250 í† í°
        # - ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸: ì•½ 50 í† í°
        # - ì´ ì…ë ¥: ì•½ 1,350 í† í°
        # - ì¶œë ¥: 500 í† í°
        # - OpenAI gpt-4o-mini ê°€ê²©: Input $0.15/1M tokens, Output $0.60/1M tokens
        # - ë¹„ìš©: (1,350/1,000,000 Ã— $0.15) + (500/1,000,000 Ã— $0.60) = $0.0002025 + $0.0003 = $0.0005025 (ì•½ 0.65ì›)
        # - í•œ ë‹¬ 10,000ì› ì˜ˆì‚°: í•˜ë£¨ ì•½ 512ê°œ ì§ˆë¬¸ ê°€ëŠ¥ (ì—¬ì „íˆ ì¶©ë¶„!)
        
        # 1. Retrieve: ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ (í•„í„° ì ìš©)
        # k=5ë¡œ ì„¤ì •í•˜ì—¬ ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸ ì œê³µ (ì •í™•ë„ í–¥ìƒ)
        filter_condition = {"doc_id": str(doc_id)} if doc_id else None
        related_docs = self.vector_store.search(query=question, k=5, filter=filter_condition) 
        
        if not related_docs:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ê³µê³ ë¬¸ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        # 2. Augment: í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        context = "\n\n".join([doc.page_content for doc in related_docs])
        
        # ë¹„ìš© ì ˆê°: í”„ë¡¬í”„íŠ¸ ê°„ì†Œí™”
        system_prompt = f"""ì•„íŒŒíŠ¸ ì²­ì•½ ê³µê³ ë¬¸ ì „ë¬¸ ë¶„ì„ AIì…ë‹ˆë‹¤. ì•„ë˜ ë‚´ìš©ë§Œ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”. ì—†ëŠ” ì •ë³´ëŠ” "ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.

[ê³µê³ ë¬¸ ë‚´ìš©]
{context}
"""

        # 3. Generate: ë‹µë³€ ìƒì„±
        try:
            if model == "gemini":
                # Gemini ëª¨ë¸ ì‚¬ìš© (ìƒˆ SDK: google-genai)
                if not genai_client:
                    return "GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ Geminië¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                
                prompt = f"{system_prompt}\n\nì§ˆë¬¸: {question}"
                
                # ì¬ì‹œë„ ë¡œì§ (429 ì—ëŸ¬ ëŒ€ì‘)
                max_retries = 3
                retry_delay = 2  # ì´ˆê¸° ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
                
                for attempt in range(max_retries):
                    try:
                        # ìƒˆ SDK ì‚¬ìš©ë²•
                        response = genai_client.models.generate_content(
                            model='gemini-2.0-flash-exp',
                            contents=prompt,
                            config={
                                'temperature': 0,
                                'max_output_tokens': 500,
                            }
                        )
                        return response.text
                        
                    except Exception as e:
                        error_msg = str(e)
                        
                        # 429 ì—ëŸ¬ ì²˜ë¦¬
                        if "429" in error_msg or "RESOURCE_EXHAUSTED" in error_msg or "quota" in error_msg.lower():
                            if attempt < max_retries - 1:
                                # ì—ëŸ¬ ë©”ì‹œì§€ì—ì„œ retry delay ì¶”ì¶œ ì‹œë„
                                retry_after = None
                                retry_match = re.search(r'retry in ([\d.]+)s', error_msg, re.IGNORECASE)
                                if retry_match:
                                    retry_after = float(retry_match.group(1))
                                else:
                                    # Exponential backoff
                                    retry_after = retry_delay * (2 ** attempt) + random.uniform(0, 1)
                                
                                print(f"âš ï¸ Gemini í• ë‹¹ëŸ‰ ì´ˆê³¼ (429). {retry_after:.1f}ì´ˆ í›„ ì¬ì‹œë„... ({attempt + 1}/{max_retries})")
                                time.sleep(retry_after)
                            else:
                                return f"ì£„ì†¡í•©ë‹ˆë‹¤. Gemini API í• ë‹¹ëŸ‰ì´ ì´ˆê³¼ë˜ì–´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                        else:
                            # ë‹¤ë¥¸ ì—ëŸ¬ëŠ” ì¦‰ì‹œ ì¬ë°œìƒ
                            raise
            else:
                # OpenAI ëª¨ë¸ ì‚¬ìš© (ê¸°ë³¸ê°’)
                response = openai.chat.completions.create(
                    model="gpt-4o-mini", # ê°€ì„±ë¹„ ì¢‹ì€ ëª¨ë¸ ì‚¬ìš©
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": question}
                    ],
                    temperature=0, # ì‚¬ì‹¤ ê¸°ë°˜ ë‹µë³€ì„ ìœ„í•´ 0 ì„¤ì •
                    max_tokens=500,  # ë‹µë³€ ê¸¸ì´ í™•ì¥ (200 â†’ 500, ê¸´ ë‹µë³€ë„ ì™„ì „íˆ ì œê³µ)
                )
                return response.choices[0].message.content
        except Exception as e:
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

    def clear_database(self):
        """ë²¡í„° DBë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        print("ğŸ—‘ï¸ ë²¡í„° DB ì´ˆê¸°í™” ìš”ì²­")
        self.vector_store.clear()

    def save_tables_to_excel(self, all_content, output_path="extracted_tables.xlsx"):
        """
        (ë””ë²„ê¹…ìš©) ì¶”ì¶œëœ í‘œ ë°ì´í„°ë¥¼ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥
        """
        with pd.ExcelWriter(output_path, engine="openpyxl") as writer:
            table_count = 0
            for item in all_content:
                if item["type"] == "table":
                    table_count += 1
                    table_data = item["content"]
                    
                    # ë°ì´í„°ê°€ ì—†ê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•Šìœ¼ë©´ íŒ¨ìŠ¤
                    if not table_data: continue
                    
                    df = pd.DataFrame(table_data)
                    sheet_name = f"Table {table_count}"
                    # ì‹œíŠ¸ ì´ë¦„ ê¸¸ì´ ì œí•œ (31ì)
                    if len(sheet_name) > 31: sheet_name = sheet_name[:31]
                    
                    try:
                        df.to_excel(writer, sheet_name=sheet_name, index=False)
                    except Exception as e:
                        print(f"ì—‘ì…€ ì‹œíŠ¸ ì €ì¥ ì‹¤íŒ¨ ({sheet_name}): {e}")

        print(f"âœ… í‘œ ë°ì´í„° ì—‘ì…€ ì €ì¥ ì™„ë£Œ: {output_path}")

    def save_rag_document_as_md(self, pdf_path: str, final_rag_document: str):
        """
        ìµœì¢… ë³€í™˜ëœ ë¬¸ì„œë¥¼ .md íŒŒì¼ë¡œ ì €ì¥
        """
        original_stem = Path(pdf_path).stem 
        md_filename = original_stem + ".md"
        output_dir = "data/md" 
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        output_path = Path(output_dir) / md_filename

        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(final_rag_document)
            print(f"âœ… ìµœì¢… Markdown ì €ì¥ ì™„ë£Œ: {output_path}")
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
