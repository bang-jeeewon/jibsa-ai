import pandas as pd
from pathlib import Path
from src.config.config import OPENAI_API_KEY, GOOGLE_API_KEY, CHUNK_BATCH_SIZE, RENDER
from openai import OpenAI
from google.genai import Client
import time
import random
import re

# ë¶„ë¦¬ëœ ëª¨ë“ˆ import
from src.services.rag.pdf_extractor import PDFExtractor
# from src.services.rag.pdf_extractor_pymupdf import PDFExtractorPyMuPDF
# from src.services.rag.pdf_extractor_llama import PDFExtractorLlama
# from src.services.rag.pdf_extractor_marker import PDFExtractorMarker
from src.services.rag.data_processor import DataProcessor
from src.services.rag.text_chunker import TextChunker
from src.services.rag.vector_store import VectorStoreService

openai = OpenAI(api_key=OPENAI_API_KEY)
genai_client = Client(api_key=GOOGLE_API_KEY) 

class RAGService:
    def __init__(self, persist_directory=None, embedding_model="openai"):
        """
        RAG íŒŒì´í”„ë¼ì¸ì„ ì´ê´„í•˜ëŠ” ì„œë¹„ìŠ¤ í´ë˜ìŠ¤.
        ETL í”„ë¡œì„¸ìŠ¤ë¥¼ ê° ë‹´ë‹¹ í´ë˜ìŠ¤ì—ê²Œ ìœ„ì„í•˜ì—¬ ì‹¤í–‰í•©ë‹ˆë‹¤.
        :param persist_directory: Noneì´ë©´ in-memory ëª¨ë“œ (íŒŒì¼ ì €ì¥ ì•ˆ í•¨, ì„œë²„ ì¬ì‹œì‘ ì‹œ ë°ì´í„° ì‚¬ë¼ì§)
        :param embedding_model: ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ ("openai" ë˜ëŠ” "gemini")
        """
        # ê° ë‹¨ê³„ë³„ ë‹´ë‹¹ì(Worker) ì´ˆê¸°í™”
        self.pdf_extractor = PDFExtractor()
        # self.pdf_extractor_pymupdf = PDFExtractorPyMuPDF()
        # self.pdf_extractor_llama = PDFExtractorLlama()
        # self.pdf_extractor_marker = PDFExtractorMarker()
        self.data_processor = DataProcessor()
        self.text_chunker = TextChunker()
        self.vector_store = VectorStoreService(persist_directory, embedding_model=embedding_model)  # None = in-memory


    def process_pdf_for_rag(self, pdf_path: str, doc_id: str):
        """
        PDF íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ RAG ì‹œìŠ¤í…œì— ì ì¬í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜ ë° ì €ì¥í•©ë‹ˆë‹¤.
        :param pdf_path: PDF íŒŒì¼ ê²½ë¡œ
        :param doc_id: ë¬¸ì„œë¥¼ ì‹ë³„í•  ìˆ˜ ìˆëŠ” ê³ ìœ  ID (ì˜ˆ: house_manage_no)
        """
        # 1. Extract: PDFì—ì„œ Raw ë°ì´í„° ì¶”ì¶œ
        print(f"ğŸ” PDF ì¶”ì¶œ ì‹œì‘: {pdf_path}")
        raw_content = self.pdf_extractor.extract_content(pdf_path)
        # raw_content = self.pdf_extractor_pymupdf.extract_content(pdf_path)
        # raw_content = self.pdf_extractor_llama.extract_content(pdf_path)
        # raw_content = self.pdf_extractor_marker.extract_content(pdf_path)
        
        # (ë””ë²„ê¹…ìš©) ì¶”ì¶œëœ í‘œ ë°ì´í„° ì—‘ì…€ ì €ì¥
        # self.save_tables_to_excel(raw_content)
        
        # 2. Transform: ë°ì´í„° ì •ì œ ë° ë§ˆí¬ë‹¤ìš´ ë³€í™˜
        print("ğŸ§¹ ë°ì´í„° ì •ì œ ë° ë³€í™˜ ì¤‘...")
        processed_docs = self.data_processor.process_content(raw_content)
        
        # raw_content ë©”ëª¨ë¦¬ í•´ì œ (ë” ì´ìƒ í•„ìš” ì—†ìŒ)
        del raw_content
        import gc
        gc.collect()
        
        # 3. ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œ ìƒì„±
        final_rag_document = "\n\n".join(processed_docs)
        
        # ë¡œì»¬ í™˜ê²½ì—ì„œë§Œ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì €ì¥
        is_render = RENDER == "true" or RENDER == "1"
        if not is_render:
            # ë¡œì»¬ í™˜ê²½: data/md/ì— ë§ˆí¬ë‹¤ìš´ ì €ì¥
            self.save_rag_document_as_md(pdf_path, final_rag_document)
        
        # processed_docs ë©”ëª¨ë¦¬ í•´ì œ
        del processed_docs
        gc.collect()
        
        # 4. Chunking: í…ìŠ¤íŠ¸ ì²­í‚¹ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ìœ„í•´ ë°°ì¹˜ë¡œ ì²˜ë¦¬)
        print("ğŸ”ª í…ìŠ¤íŠ¸ ì²­í‚¹ ì¤‘...")
        chunks = self.text_chunker.chunk_markdown(final_rag_document)
        
        # final_rag_document ë©”ëª¨ë¦¬ í•´ì œ (ì²­í‚¹ ì™„ë£Œ í›„ ë” ì´ìƒ í•„ìš” ì—†ìŒ)
        del final_rag_document
        gc.collect()
        
        # [ì¤‘ìš”] ëª¨ë“  ì²­í¬ì— ë¬¸ì„œ ID(doc_id) ë©”íƒ€ë°ì´í„° ì¶”ê°€
        for chunk in chunks:
            chunk.metadata['doc_id'] = str(doc_id)

        print(f"âœ… ì´ {len(chunks)}ê°œì˜ ì²­í¬ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # 5. Load: ë²¡í„° DB ì €ì¥
        if chunks:
            # Render í™˜ê²½ì¸ì§€ í™•ì¸ (í™˜ê²½ ë³€ìˆ˜ë¡œ êµ¬ë¶„)
            # CHUNK_BATCH_SIZEê°€ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´ ê·¸ ê°’ì„ ì‚¬ìš©
            # ì—†ìœ¼ë©´ RENDER í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì—¬ ê²°ì •
            chunk_batch_size_env = CHUNK_BATCH_SIZE
            
            if chunk_batch_size_env is not None:
                # ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •ëœ ê²½ìš°
                chunk_batch_size = int(chunk_batch_size_env)
            else:
                # í™˜ê²½ ë³€ìˆ˜ê°€ ì—†ìœ¼ë©´ RENDER í™˜ê²½ í™•ì¸
                is_render = RENDER == "true" or RENDER == "1"
                chunk_batch_size = 50 if is_render else 0  # Renderë©´ 50, ë¡œì»¬ì´ë©´ 0 (í•œ ë²ˆì— ì²˜ë¦¬)
            
            # ë°°ì¹˜ í¬ê¸°ê°€ 0ì´ê±°ë‚˜ ì²­í¬ ê°œìˆ˜ë³´ë‹¤ í¬ë©´ í•œ ë²ˆì— ì²˜ë¦¬ (ë¡œì»¬ í™˜ê²½)
            if chunk_batch_size == 0 or chunk_batch_size >= len(chunks):
                print(f"  ğŸ’¾ ì²­í¬ ì €ì¥ ì¤‘... (í•œ ë²ˆì— {len(chunks)}ê°œ)")
                self.vector_store.add_documents(chunks)
            else:
                # Render í™˜ê²½: ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì €ì¥ (ë©”ëª¨ë¦¬ íš¨ìœ¨)
                total_chunks = len(chunks)
                print(f"  ğŸ’¾ ì²­í¬ ë°°ì¹˜ ì €ì¥ ì¤‘... (ë°°ì¹˜ í¬ê¸°: {chunk_batch_size})")
                
                for i in range(0, total_chunks, chunk_batch_size):
                    batch = chunks[i:i + chunk_batch_size]
                    print(f"    ë°°ì¹˜ {i//chunk_batch_size + 1}/{(total_chunks + chunk_batch_size - 1)//chunk_batch_size} ì €ì¥ ì¤‘...")
                    self.vector_store.add_documents(batch)
                    # ë°°ì¹˜ ì €ì¥ í›„ ë©”ëª¨ë¦¬ í•´ì œ
                    del batch
                    gc.collect()
            
            # ëª¨ë“  ì²­í¬ ë©”ëª¨ë¦¬ í•´ì œ
            del chunks
            gc.collect()

        return '====ì²˜ë¦¬ ì™„ë£Œ===='

    def answer_question(self, question: str, doc_id: str = None, model: str = "openai"):
        """
        ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ RAG ë°©ì‹ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
        :param doc_id: íŠ¹ì • ë¬¸ì„œì—ì„œë§Œ ê²€ìƒ‰í•˜ë ¤ë©´ ID ì§€ì •
        :param model: ì‚¬ìš©í•  ëª¨ë¸ ("openai" ë˜ëŠ” "gemini")
        """
        model_display = "GPT-4o-mini" if model == "openai" else "Gemini Pro"
        print(f"ğŸ¤” ì§ˆë¬¸ ë¶„ì„ ì¤‘: {question}")
        print(f"ğŸ“‹ ë¬¸ì„œ ID: {doc_id}, ì„ íƒëœ LLM: {model_display}")
        # 1íšŒ ì§ˆë¬¸ ë¹„ìš© ê³„ì‚° (k=5 ê¸°ì¤€):
        # - ì§ˆë¬¸: ì•½ 50 í† í°
        # - ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ (k=5): ì²­í¬ 5ê°œ Ã— 250 í† í° = ì•½ 1,250 í† í°
        # - ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸: ì•½ 50 í† í°
        # - ì´ ì…ë ¥: ì•½ 1,350 í† í°
        # - ì¶œë ¥: 500 í† í°
        # - OpenAI gpt-4o-mini ê°€ê²©: Input $0.15/1M tokens, Output $0.60/1M tokens
        # - ë¹„ìš©: (1,350/1,000,000 Ã— $0.15) + (500/1,000,000 Ã— $0.60) = $0.0002025 + $0.0003 = $0.0005025 (ì•½ 0.65ì›)
        # - í•œ ë‹¬ 10,000ì› ì˜ˆì‚°: í•˜ë£¨ ì•½ 512ê°œ ì§ˆë¬¸ ê°€ëŠ¥ (ì—¬ì „íˆ ì¶©ë¶„!)
        
        # 1. Retrieve: ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ (í•„í„° ì ìš©)
        # k=10ìœ¼ë¡œ ì¦ê°€í•˜ì—¬ í‘œ ë°ì´í„° ë“± ë‹¤ì–‘í•œ í˜•ì‹ì˜ ì •ë³´ë„ í¬í•¨
        filter_condition = {"doc_id": str(doc_id)} if doc_id else None
        related_docs = self.vector_store.search(query=question, k=10, filter=filter_condition) 
        
        if not related_docs:
            print("âš ï¸ ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë²¡í„° DBì— ë°ì´í„°ê°€ ì €ì¥ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ê³µê³ ë¬¸ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ê³µê³ ë¬¸ì„ ë¶„ì„í•´ì£¼ì„¸ìš”."

        # 2. Augment: í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        context = "\n\n".join([doc.page_content for doc in related_docs])
        
        # ë””ë²„ê¹…: ê²€ìƒ‰ëœ ë¬¸ì„œ ì •ë³´ ì¶œë ¥
        print(f"ğŸ“„ ê²€ìƒ‰ëœ ë¬¸ì„œ ê°œìˆ˜: {len(related_docs)}")
        for i, doc in enumerate(related_docs[:2], 1):  # ì²˜ìŒ 2ê°œë§Œ ì¶œë ¥
            preview = doc.page_content[:500].replace('\n', ' ')
            print(f"  ë¬¸ì„œ {i} (ë¯¸ë¦¬ë³´ê¸°): {preview}...")
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ë” ëª…í™•í•œ ì§€ì‹œì‚¬í•­)
        system_prompt = f"""ë‹¹ì‹ ì€ ì•„íŒŒíŠ¸ ì²­ì•½ ê³µê³ ë¬¸ì„ ì „ë¬¸ì ìœ¼ë¡œ ë¶„ì„í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì•„ë˜ [ê³µê³ ë¬¸ ë‚´ìš©] ì„¹ì…˜ì— ìˆëŠ” ì •ë³´ë§Œì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

**ë‹µë³€ ê·œì¹™:**
1. ê³µê³ ë¬¸ ë‚´ìš©ì— ëª…í™•íˆ ë‚˜ì™€ìˆëŠ” ì •ë³´ë§Œ ë‹µë³€í•˜ì„¸ìš”.
2. ì •ë³´ê°€ ì—†ê±°ë‚˜ ë¶ˆí™•ì‹¤í•œ ê²½ìš° "ê³µê³ ë¬¸ì— í•´ë‹¹ ì •ë³´ê°€ ëª…ì‹œë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.
3. ê°€ëŠ¥í•œ í•œ êµ¬ì²´ì ì´ê³  ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš” (ìˆ«ì, ë‚ ì§œ, ì¡°ê±´ ë“±).
4. ì—¬ëŸ¬ í•­ëª©ì´ ìˆëŠ” ê²½ìš° ëª©ë¡ìœ¼ë¡œ ì •ë¦¬í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
5. **í‘œ í˜•ì‹ì˜ ë°ì´í„°ë¥¼ ì£¼ì˜ ê¹Šê²Œ í™•ì¸í•˜ì„¸ìš”.** í‘œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
   - ì˜ˆ: "ì „ë§¤ì œí•œ ê¸°ê°„"ì„ ë¬»ëŠ” ê²½ìš°, í‘œì—ì„œ "ì „ë§¤ì œí•œ" ì—´ì„ ì°¾ì•„ë³´ì„¸ìš”.
   - í‘œì˜ í—¤ë”ì™€ ê°’ì„ ë§¤ì¹­í•˜ì—¬ ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.

[ê³µê³ ë¬¸ ë‚´ìš©]
{context}
"""

        # 3. Generate: ë‹µë³€ ìƒì„±
        try:
            if model == "gemini":
                # Gemini ëª¨ë¸ ì‚¬ìš© (ìƒˆ SDK: google-genai)
                if not genai_client:
                    return "GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ Geminië¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                
                prompt = f"{system_prompt}\n\nì§ˆë¬¸: {question}"
                
                # ì¬ì‹œë„ ë¡œì§ (429 ì—ëŸ¬ ëŒ€ì‘)
                max_retries = 3
                retry_delay = 2  # ì´ˆê¸° ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
                
                for attempt in range(max_retries):
                    try:
                        # ìƒˆ SDK ì‚¬ìš©ë²•
                        response = genai_client.models.generate_content(
                            model='gemini-3-pro-preview',
                            contents=prompt,
                            config={
                                'temperature': 0,
                                'max_output_tokens': 2000,  # 500 â†’ 2000ìœ¼ë¡œ ì¦ê°€ (MAX_TOKENS ì—ëŸ¬ ë°©ì§€)
                            }
                        )
                        
                        # response.textê°€ Noneì¸ ê²½ìš° ì²˜ë¦¬
                        if response.text is None:
                            # finish_reason í™•ì¸
                            if hasattr(response, 'candidates') and response.candidates:
                                candidate = response.candidates[0]
                                finish_reason = getattr(candidate, 'finish_reason', None)
                                print(f"âš ï¸ Gemini ì‘ë‹µì´ Noneì…ë‹ˆë‹¤. finish_reason: {finish_reason}")
                                
                                if finish_reason == 'MAX_TOKENS':
                                    print("âš ï¸ ìµœëŒ€ í† í° ìˆ˜ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. max_output_tokensë¥¼ ëŠ˜ë ¤ì•¼ í•©ë‹ˆë‹¤.")
                                    return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì´ ë„ˆë¬´ ê¸¸ì–´ì„œ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ í•´ì£¼ì„¸ìš”."
                            
                            if attempt < max_retries - 1:
                                print(f"  ì¬ì‹œë„í•©ë‹ˆë‹¤... ({attempt + 1}/{max_retries})")
                                time.sleep(2)
                                continue
                            else:
                                return "ì£„ì†¡í•©ë‹ˆë‹¤. Geminiê°€ ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                        
                        return f"Gemini 3 Pro: {response.text}"
                        
                    except Exception as e:
                        error_msg = str(e)
                        
                        # 429 ì—ëŸ¬ ì²˜ë¦¬
                        if "429" in error_msg or "RESOURCE_EXHAUSTED" in error_msg or "quota" in error_msg.lower():
                            if attempt < max_retries - 1:
                                # ì—ëŸ¬ ë©”ì‹œì§€ì—ì„œ retry delay ì¶”ì¶œ ì‹œë„
                                retry_after = None
                                retry_match = re.search(r'retry in ([\d.]+)s', error_msg, re.IGNORECASE)
                                if retry_match:
                                    retry_after = float(retry_match.group(1))
                                else:
                                    # Exponential backoff
                                    retry_after = retry_delay * (2 ** attempt) + random.uniform(0, 1)
                                
                                print(f"âš ï¸ Gemini í• ë‹¹ëŸ‰ ì´ˆê³¼ (429). {retry_after:.1f}ì´ˆ í›„ ì¬ì‹œë„... ({attempt + 1}/{max_retries})")
                                time.sleep(retry_after)
                            else:
                                return f"ì£„ì†¡í•©ë‹ˆë‹¤. Gemini API í• ë‹¹ëŸ‰ì´ ì´ˆê³¼ë˜ì–´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                        else:
                            # ë‹¤ë¥¸ ì—ëŸ¬ëŠ” ì¦‰ì‹œ ì¬ë°œìƒ
                            raise
            else:
                # OpenAI ëª¨ë¸ ì‚¬ìš© (ê¸°ë³¸ê°’)
                response = openai.chat.completions.create(
                    model="gpt-4o-mini", # ê°€ì„±ë¹„ ì¢‹ì€ ëª¨ë¸ ì‚¬ìš©
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": question}
                    ],
                    temperature=0, # ì‚¬ì‹¤ ê¸°ë°˜ ë‹µë³€ì„ ìœ„í•´ 0 ì„¤ì •
                    max_tokens=1000,  # ë‹µë³€ ê¸¸ì´ í™•ì¥ (200 â†’ 500, ê¸´ ë‹µë³€ë„ ì™„ì „íˆ ì œê³µ)
                )
                return f"GPT-4o-mini: {response.choices[0].message.content}"
        except Exception as e:
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

    def clear_database(self):
        """ë²¡í„° DBë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        print("ğŸ—‘ï¸ ë²¡í„° DB ì´ˆê¸°í™” ìš”ì²­")
        self.vector_store.clear()

    def save_tables_to_excel(self, all_content, output_path="extracted_tables.xlsx"):
        """
        (ë””ë²„ê¹…ìš©) ì¶”ì¶œëœ í‘œ ë°ì´í„°ë¥¼ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥
        """
        with pd.ExcelWriter(output_path, engine="openpyxl") as writer:
            table_count = 0
            for item in all_content:
                if item["type"] == "table":
                    table_count += 1
                    table_data = item["content"]
                    
                    # ë°ì´í„°ê°€ ì—†ê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•Šìœ¼ë©´ íŒ¨ìŠ¤
                    if not table_data: continue
                    
                    df = pd.DataFrame(table_data)
                    sheet_name = f"Table {table_count}"
                    # ì‹œíŠ¸ ì´ë¦„ ê¸¸ì´ ì œí•œ (31ì)
                    if len(sheet_name) > 31: sheet_name = sheet_name[:31]
                    
                    try:
                        df.to_excel(writer, sheet_name=sheet_name, index=False)
                    except Exception as e:
                        print(f"ì—‘ì…€ ì‹œíŠ¸ ì €ì¥ ì‹¤íŒ¨ ({sheet_name}): {e}")

        print(f"âœ… í‘œ ë°ì´í„° ì—‘ì…€ ì €ì¥ ì™„ë£Œ: {output_path}")

    def save_rag_document_as_md(self, pdf_path: str, final_rag_document: str):
        """
        ìµœì¢… ë³€í™˜ëœ ë¬¸ì„œë¥¼ .md íŒŒì¼ë¡œ ì €ì¥
        """
        original_stem = Path(pdf_path).stem 
        md_filename = original_stem + ".md"
        output_dir = "data/md" 
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        output_path = Path(output_dir) / md_filename

        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(final_rag_document)
            print(f"âœ… ìµœì¢… Markdown ì €ì¥ ì™„ë£Œ: {output_path}")
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
